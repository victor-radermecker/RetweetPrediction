{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First let's clean the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>user_verified</th>\n",
       "      <th>user_statuses_count</th>\n",
       "      <th>user_followers_count</th>\n",
       "      <th>user_friends_count</th>\n",
       "      <th>user_mentions</th>\n",
       "      <th>urls</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1588696955143</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>68460</td>\n",
       "      <td>1101</td>\n",
       "      <td>1226</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Smh I give up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1588464948124</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>309</td>\n",
       "      <td>51</td>\n",
       "      <td>202</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Most of us are Human Beings, but I think you m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1588634673360</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>3241</td>\n",
       "      <td>1675</td>\n",
       "      <td>2325</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Old dirty tricks Trump, at it again...like we ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1588433158672</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32327</td>\n",
       "      <td>667</td>\n",
       "      <td>304</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Seriously..... I worked 86 hours my last check...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1588582751599</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>581</td>\n",
       "      <td>42</td>\n",
       "      <td>127</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>May ALMIGHTY ALLAH have mercy on us all. Only ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id      timestamp  retweet_count  user_verified  user_statuses_count  \\\n",
       "0   0  1588696955143              0          False                68460   \n",
       "1   1  1588464948124              0          False                  309   \n",
       "2   2  1588634673360              0          False                 3241   \n",
       "3   3  1588433158672              0          False                32327   \n",
       "4   4  1588582751599              0          False                  581   \n",
       "\n",
       "   user_followers_count  user_friends_count user_mentions urls hashtags  \\\n",
       "0                  1101                1226           NaN  NaN      NaN   \n",
       "1                    51                 202           NaN  NaN      NaN   \n",
       "2                  1675                2325           NaN  NaN      NaN   \n",
       "3                   667                 304           NaN  NaN      NaN   \n",
       "4                    42                 127           NaN  NaN      NaN   \n",
       "\n",
       "                                                text  \n",
       "0                                      Smh I give up  \n",
       "1  Most of us are Human Beings, but I think you m...  \n",
       "2  Old dirty tricks Trump, at it again...like we ...  \n",
       "3  Seriously..... I worked 86 hours my last check...  \n",
       "4  May ALMIGHTY ALLAH have mercy on us all. Only ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function used to add the hashtags from text to 'hashtags' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hashtag(row):\n",
    "    return \" \".join(filter(lambda x:x[0]!='#', row.split()))\n",
    "\n",
    "def delete_duplicates(string):\n",
    "    words = string.split(',')\n",
    "    words = list(map(str.strip, words))\n",
    "    return \" \".join(sorted(set(words), key=words.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove hashtags from text\n",
    "def find_deplace_hashtags(df):\n",
    "    '''\n",
    "    This function moves the \"#\" from the text directly to the 'hashtags' columns.\n",
    "    '''\n",
    "    #Find extra hashtags\n",
    "    extra_hashtags = df.apply(lambda x: re.findall(\"[#]\\w+\", x.text), axis=1)\n",
    "    \n",
    "    clean_hashtags = []\n",
    "    for ls in extra_hashtags:\n",
    "        res = ', '.join(ls)\n",
    "        res = res.replace('#','')\n",
    "        clean_hashtags.append(res)\n",
    "    \n",
    "    print(clean_hashtags)\n",
    "    df.text = df.apply(lambda x: clean_hashtag(x.text), axis=1)\n",
    "    \n",
    "    #add them to hashtags columns\n",
    "    df['hashtags'] = df['hashtags'] + ', ' + pd.Series(clean_hashtags)\n",
    "    df.hashtags = df.apply(lambda x : delete_duplicates(str(x.hashtags)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COVID19\n",
      "Thank you to all of the nurses in our @StanfordEMED family sharing the #COVID19 frontlines. We are definitely #strongertogether because of you! https://t.co/X1XaJjhbsO\n",
      "twitter.com/i/web/status/1â€¦\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[665773].hashtags)\n",
    "print(df.iloc[665773].text)\n",
    "print(df.iloc[665773].urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "find_deplace_hashtags(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COVID19 strongertogether\n",
      "Thank you to all of the nurses in our @StanfordEMED family sharing the frontlines. We are definitely because of you! https://t.co/X1XaJjhbsO\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[665773].hashtags)\n",
    "print(df.iloc[665773].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_urls(row):\n",
    "    return \" \".join(filter(lambda x:x[0:5]!='https', row.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove URLS\n",
    "def find_deplace_urls(df):\n",
    "    '''\n",
    "    This function moves the \"#\" from the text directly to the 'hashtags' columns.\n",
    "    '''\n",
    "    #Find extra hashtags\n",
    "    extra_urls = df.apply(lambda x: re.findall(r\"https.*\", x.text), axis=1)\n",
    "    \n",
    "    clean_urls_res = []\n",
    "    for ls in extra_urls:\n",
    "        res = ', '.join(ls)\n",
    "        res = res.replace('nan, ', '')\n",
    "        clean_urls_res.append(res)\n",
    "   \n",
    "    df.text = df.apply(lambda x: clean_urls(x.text), axis=1)\n",
    "        \n",
    "    #add them to urls columns  \n",
    "    df.urls = df.apply(lambda x : delete_duplicates(str(x.urls)), axis=1)\n",
    "    df[\"urls\"] = df[\"urls\"].str.cat(clean_urls_res, sep=', ')\n",
    "    df.urls = df.urls.apply(lambda x: x.replace('nan, ', ''))\n",
    "    df.urls = df.urls.replace('nan, ',np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "find_deplace_urls(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you to all of the nurses in our @StanfordEMED family sharing the frontlines. We are definitely because of you!\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[665773].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the user_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_at(row):\n",
    "    return \" \".join(filter(lambda x:x[0]!='@', row.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove hashtags from text\n",
    "def find_deplace_ats(df):\n",
    "    '''\n",
    "    This function moves the \"@\" from the text directly to the 'hashtags' columns.\n",
    "    '''\n",
    "    #Find extra hashtags\n",
    "    extra_hashtags = df.apply(lambda x: re.findall(\"[@]\\w+\", x.text), axis=1)\n",
    "    \n",
    "    clean_ats = []\n",
    "    for ls in extra_hashtags:\n",
    "        res = ', '.join(ls)\n",
    "        res = res.replace('@','')\n",
    "        clean_ats.append(res)\n",
    "    \n",
    "    df.text = df.apply(lambda x: clean_at(x.text), axis=1)\n",
    "    \n",
    "    #add them to hashtags columns\n",
    "    df['user_mentions'] = df['user_mentions'] + ', ' + pd.Series(clean_ats)\n",
    "    df.user_mentions = df.apply(lambda x : delete_duplicates(str(x.user_mentions)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_deplace_ats(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After #hashtags, @user_mentions and https links, we have only sentences in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COVID19 strongertogether\n",
      "'Post it' pearls for Palliative, End of Life and Bereavement care in COVID-19 up the ED. Relating to symptom management, considerations for before and after death, honest conversations and goal setting\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[665773].hashtags)\n",
    "print(df.iloc[665774].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving new csv\n",
    "df.to_csv('../../data/train_cleanV1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-13a399a4795b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m665773\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "print(df.iloc[665773].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from datetime import datetime\n",
    "#df_2['timestamp'] = df['timestamp'].apply(lambda x: datetime.utcfromtimestamp(int(str(x))/1000).strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dealing with timestamps\n",
    "import pandas as pd\n",
    "df = pd.read_csv('../../data/train_cleanV4.csv')\n",
    "df.drop(['Unnamed: 0', 'Unnamed: 0.1'], axis=1, inplace=True)\n",
    "df.timestamp = pd.to_datetime(df.timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hour'] = df.timestamp.dt.hour\n",
    "df['date'] = df.timestamp.dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../../data/train_cleanV4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INF554",
   "language": "python",
   "name": "inf554"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
